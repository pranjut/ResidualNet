{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import math\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranjut/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import reduce_sum\n",
    "from tensorflow.keras.backend import pow\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEEL_PATH=\"dataset\"\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "SUBMIT_SAMPLE = \"sample_submission.csv\"\n",
    "TRAIN_IMAGE=STEEL_PATH + \"/train_images\"\n",
    "TEST_IMAGE=STEEL_PATH + \"/test_images/\"\n",
    "PRETRAINED_MODEL=STEEL_PATH + '/saved_model/ResNet_network.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    conv = tf.keras.layers.Activation('relu')(x)\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    res = conv_block(x, filters, 3, padding, strides)\n",
    "    res = conv_block(res, filters, 3, padding, 1)\n",
    "    skip = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    skip = tf.keras.layers.BatchNormalization()(skip)\n",
    "    output = Add()([skip, res])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_act(x, act=True):\n",
    "    'batch normalization layer with an optinal activation layer'\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if act == True:\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x, filters, kernel_size=3, padding='same', strides=1):\n",
    "    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n",
    "    conv = conv_block(conv, filters, kernel_size, padding, strides)\n",
    "    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n",
    "    shortcut = bn_act(shortcut, act=False)\n",
    "    output = Add()([conv, shortcut])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_concat_block(x, xskip):\n",
    "    u = UpSampling2D((2,2))(x)\n",
    "    c = Concatenate()([u, xskip])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(img_h, img_w):\n",
    "    f = [16, 32, 64, 128, 256]\n",
    "    inputs = Input((img_h, img_w, 1))\n",
    "    \n",
    "    ## Encoder\n",
    "    e0 = inputs\n",
    "    e1 = stem(e0, f[0])\n",
    "    e2 = residual_block(e1, f[1], strides=2)\n",
    "    e3 = residual_block(e2, f[2], strides=2)\n",
    "    e4 = residual_block(e3, f[3], strides=2)\n",
    "    e5 = residual_block(e4, f[4], strides=2)\n",
    "    \n",
    "    ## Bridge\n",
    "    b0 = conv_block(e5, f[4], strides=1)\n",
    "    b1 = conv_block(b0, f[4], strides=1)\n",
    "    \n",
    "    ## Decoder\n",
    "    u1 = upsample_concat_block(b1, e4)\n",
    "    d1 = residual_block(u1, f[4])\n",
    "    \n",
    "    u2 = upsample_concat_block(d1, e3)\n",
    "    d2 = residual_block(u2, f[3])\n",
    "    \n",
    "    u3 = upsample_concat_block(d2, e2)\n",
    "    d3 = residual_block(u3, f[2])\n",
    "    \n",
    "    u4 = upsample_concat_block(d3, e1)\n",
    "    d4 = residual_block(u4, f[1])\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(4, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tversky(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_pos = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n",
    "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def focal_tversky_loss(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return tf.keras.backend.pow((1-pt_1), gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w = 800\n",
    "img_h = 256\n",
    "model = network(img_h=img_h, img_w=img_w)\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
    "model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(steel_path=STEEL_PATH, file_name=TRAIN_CSV):\n",
    "    csv_path = os.path.join(steel_path, file_name)\n",
    "    return pd.read_csv(csv_path).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_to_mask(rle_string,height,width):\n",
    "        '''\n",
    "        convert RLE(run length encoding) string to numpy array\n",
    "\n",
    "        Parameters: \n",
    "        rleString (str): Description of arg1 \n",
    "        height (int): height of the mask\n",
    "        width (int): width of the mask \n",
    "\n",
    "        Returns: \n",
    "        numpy.array: numpy array of the mask\n",
    "        '''\n",
    "        rows, cols = height, width\n",
    "        if rle_string == -1:\n",
    "            return np.zeros((height, width))\n",
    "        else:\n",
    "            rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n",
    "            rlePairs = np.array(rleNumbers).reshape(-1,2)\n",
    "            img = np.zeros(rows*cols,dtype=np.uint8)\n",
    "            for index,length in rlePairs:\n",
    "                index -= 1\n",
    "                img[index:index+length] = 255\n",
    "            img = img.reshape(cols,rows)\n",
    "            img = img.T\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, list_ids, labels, image_dir, batch_size=32,\n",
    "                 img_h=256, img_w=512, shuffle=True):\n",
    "        \n",
    "        self.list_ids = list_ids\n",
    "        self.labels = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        'denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_ids)) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'generate one batch of data'\n",
    "        print(\"Generating data .....\")\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # get list of IDs\n",
    "        list_ids_temp = [self.list_ids[k] for k in indexes]\n",
    "        # generate data\n",
    "        X, y = self.__data_generation(list_ids_temp)\n",
    "        # return data \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'update ended after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_ids))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, list_ids_temp):\n",
    "        'generate data containing batch_size samples'\n",
    "        X = np.empty((self.batch_size, self.img_h, self.img_w, 1))\n",
    "        y = np.empty((self.batch_size, self.img_h, self.img_w, 4))\n",
    "        \n",
    "        for idx, id in enumerate(list_ids_temp):\n",
    "            file_path =  os.path.join(self.image_dir, id)\n",
    "            image = cv2.imread(file_path, 0)\n",
    "            image_resized = cv2.resize(image, (self.img_w, self.img_h))\n",
    "            image_resized = np.array(image_resized, dtype=np.float64)\n",
    "            # standardization of the image\n",
    "            image_resized -= image_resized.mean()\n",
    "            image_resized /= image_resized.std()\n",
    "            \n",
    "            mask = np.empty((img_h, img_w, 4))\n",
    "            \n",
    "            for idm, image_class in enumerate(['1','2','3','4']):\n",
    "                rle = self.labels.get(id + '_' + image_class)\n",
    "                # if there is no mask create empty mask\n",
    "                if rle is None:\n",
    "                    class_mask = np.zeros((1600, 256))\n",
    "                else:\n",
    "                    class_mask = rle_to_mask(rle, width=1600, height=256)\n",
    "             \n",
    "                class_mask_resized = cv2.resize(class_mask, (self.img_w, self.img_h))\n",
    "                mask[...,idm] = class_mask_resized\n",
    "            \n",
    "            X[idx,] = np.expand_dims(image_resized, axis=2)\n",
    "            y[idx,] = mask\n",
    "        \n",
    "        # normalize Y\n",
    "        y = (y > 0).astype(int)     \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data()\n",
    "image_ids_with_duplicats = train_df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "# print(type(image_ids_with_duplicats))\n",
    "image_ids = image_ids_with_duplicats.unique()\n",
    "# train_df[\"ImageId_ClassId\"].head()\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8797"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_rest = train_test_split(image_ids, test_size=.30, random_state=42)\n",
    "# X_val, X_test = train_test_split(X_rest, test_size=.50, random_state=42)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['33514c0b1.jpg', '2d1658337.jpg', 'fbf282a60.jpg', ...,\n",
       "       '6cf49f691.jpg', '11b1e2910.jpg', '93c5acf9d.jpg'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'img_h': img_h,\n",
    "          'img_w': img_w,\n",
    "          'image_dir': TRAIN_IMAGE,\n",
    "          'batch_size': 12,\n",
    "          'shuffle': True}\n",
    "\n",
    "labels = {}\n",
    "# print(labels)\n",
    "for index, row in train_df[train_df['EncodedPixels']!=-1].iterrows():\n",
    "    labels[row['ImageId_ClassId']] = row['EncodedPixels']\n",
    "    \n",
    "\n",
    "# for key in labels.keys():\n",
    "#     print(labels[key])\n",
    "\n",
    "# Get Generators\n",
    "training_generator = DataGenerator(X_train, labels, **params)\n",
    "validation_generator = DataGenerator(X_rest, labels, **params)\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
